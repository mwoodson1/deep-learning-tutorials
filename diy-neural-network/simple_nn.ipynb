{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY Neural Network\n",
    "For this first deep learning related blog post I will follow the tradition of deep learning bloggers doing a basic neural network for their first blog. The tradition is just though as it is a good exercise I believe everyone should do at least once. Even though it seems routine I hope though that I can present a different perspective than most others. In particular I will stray from any brain or neuron related analogies and explain the concepts more mathematically. \n",
    "\n",
    "I will do a few derivations but a large portion of the math will be left to the reader. Due to this I expect at least basic understanding of linear algebra, calculus, and some of the basics of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our Libraries\n",
    "I will primarily use the Numpy library in this and future tutorials. If you are unfamiliar with Numpy I suggest you take a look at some of the following references:\n",
    "\n",
    "\n",
    "I also use Matplotlib which has syntax very similar to that used in Matlab. scikit-learn is only used for importing our dataset but I encourage you to play around with other datasets to see how this code performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset\n",
    "We will be working with scikit-learn's MNIST dataset in this tutorial. This dataset consists of a set of images of digits 0-9 in the form of a 8x8 array or 64 total pixels. This dataset is commonly used to benchmark neural network performance in the literature so it is good to get to know it now rather than later.\n",
    "\n",
    "Before doing any kind of processing or running any machine learning algorithms you should always give at least a cursory glance at your data. In this case I am very familiar with this MNIST dataset and know the distribution of classes is relatively uniform, an image cannot have multiple labels, and other properties which are important in choosing your machine learning algorithm of choice. Because of this prior knowledge I only show what the images look like but I encourage you to get to know your data before simply throwing deep learning at it! In some cases you can get away with much simpler algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7ff5b0372c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFsCAYAAACJh6H7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5b08f0b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "plt.matshow(digits.images[0],cmap='gray') \n",
    "\n",
    "plt.title('MNIST Example Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split our dataset into a train and test set. We will use 80% of the data for training and 20% for testing. In practice you should also use a validation set but for the purpose of this tutorial a train and test is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (1437, 10)\n",
      "(360, 64) (360, 10)\n"
     ]
    }
   ],
   "source": [
    "num_examples, data_dim = X_digits.shape\n",
    "num_train_examples = int(0.8*num_examples)\n",
    "num_test_exmaples = num_examples - num_train_examples\n",
    "\n",
    "#shuffle our data\n",
    "X,y = shuffle(X_digits,y_digits)\n",
    "\n",
    "#one-hot encode our labels\n",
    "y_enc = np.zeros((num_examples,10))\n",
    "y_enc[np.arange(num_examples), y] = 1\n",
    "\n",
    "#split our data\n",
    "X_train = X[0:num_train_examples,:]\n",
    "y_train = y_enc[0:num_train_examples,:]\n",
    "\n",
    "X_test = X[num_train_examples:,:]\n",
    "y_test = y_enc[num_train_examples:,:]\n",
    "\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "\n",
    "### The Architecture\n",
    "We will start with the simplest neural network of 3 layers; input, hidden, and output. Our input size will be the dimension of our data (64) which is the number of pixels in our image, our output size will be 10 since there are 10 possible digits, and our hidden size will be left up to us. We can see a general picture of this below\n",
    "\n",
    "![Simple Neural Network Architecture](img/nn.jpeg)\n",
    "\n",
    "### The Activation Function\n",
    "Once we have the architecture of our network decided on we need a few more pieces before we can train it. One of the most crucial pieces is the activation function which adds a non-linearity to our network. We will see later that without this non-linearity the entire network can be compressed into a single matrix multiplication. The most common activation function in the current literature is [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) but historically it has been [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). For this tutorial I will stick with sigmoid due to its historical importance and ease of understanding the math of backpropogation.\n",
    "\n",
    "$$ReLU(x) = max(0,x)$$\n",
    "![ReLU Activation Function](img/relu.svg)\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "![Sigmoid Activation Function](img/sigmoid.png)\n",
    "\n",
    "### The Output\n",
    "Depending on your task you might also need to change the activation function of the output layer as well. In our case since we are performing a classification task we want each node to represent a probability of being that digit. The [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation function does exactly this. Softmax  ensures that the output layer follows the rules of being a probability distribution. The exact formular for this is shown below.\n",
    "\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{k=1}^K e^{x_k}}$$\n",
    "\n",
    "Below we define our activation function and our output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    '''\n",
    "    x: vector input to activation function\n",
    "    deriv: indicator for whether we are calculating \n",
    "           the derivative of sigmoid\n",
    "           \n",
    "    The deriv parameter will be helpful in the backpropogation \n",
    "    calculations.\n",
    "    '''\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    x: vector input from the last layer of our network\n",
    "    \n",
    "    Here I use a mathematical trick to ensure the calculations\n",
    "    are easier to compute. You can test yourself by ensuring that\n",
    "    this code correctly implements the formula above.\n",
    "    '''\n",
    "    e = np.exp(x - np.max(x,axis=1,keepdims=True))\n",
    "    return e / (np.sum(e,axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "Now that we have defined our network we can begin to look at how exactly this structure can be used for classification and how to train such a model. We will start with the easy work first which is the classification or \"feed-forward\" step.\n",
    "\n",
    "### Forward Propogation\n",
    "Let $x$ be our input vector and $\\hat{y}$ be our output vector. For the forward propogation step we are simply concatenating a series of linear classifiers with a non-linearity. If you are familiar with [logistic regression] you can consider each layer performing a logistic regression followed by a non-linearity. Below is the exact definition of forward propogation\n",
    "\n",
    "$$\\begin{align}\n",
    "  z_1 &= xW_1 + b_1  \\\\\n",
    "  a_1 &= \\sigma (z_1) \\\\\n",
    "  z_2 &= a_1W_2 + b_2 \\\\\n",
    "  \\hat{y}a_2 &= softmax(z_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $W_1$ is a matrix of weights representing the weighted connections between the input and hidden layer, $b_1$ is a bias term, $W_2$ is the matrix of weights connecting the hidden and output layer, and $b_2$ is the bias term for the last layer.\n",
    "\n",
    "Note that these formulas will vary depending on your non-linearity and number of layers but the mathematics behind it does not change. We can also see from these formulas why a non-linearity is important as without it we could simply compact all the computation into a single matrix multiplication which would result in simple logistic regression.\n",
    "\n",
    "### Backpropogation\n",
    "Now we get to the real meat of this tutorial which is backpropogation. The goal of backpropogation is to minimize the error in our training data. A common error function to use is [cross-entropy]() which we define below.\n",
    "\n",
    "$$L(y,\\hat{y}) = -\\frac{1}{N}\\sum_{n\\in N}\\sum_{i\\in C} y_{n,i}log(\\hat{y}_{n,i})$$\n",
    "\n",
    "Now we have everything we need to start our derivation of backpropogation. The key trick to backpropogation is knowing that we will be optimizing this network with gradient descent. Knowing this all we need to be able to compute is the gradient of the loss with respect to all of the parameters of the network. With this we can make a \"step\" in the right direction during the optimization process. \n",
    "\n",
    "The reason this algorithm is called backpropogation is the direction in which we go through the network when calculating the gradients is in reverse to the direction we took during forward propogation. Knowing this we first calculate the derivative of the error with respect to the output of the last layer. The derivation for this particular step is rather cumbersome but leads to a elegant solution\n",
    "\n",
    "$$\\delta_3 = \\frac{\\partial L}{\\partial z_2} = y-\\hat{y}$$\n",
    "\n",
    "Let us also define\n",
    "\n",
    "$$\\delta_2 = \\frac{\\partial L}{\\partial z_1}$$\n",
    "\n",
    "which translates to the derivative of the loss with respect to the output of layer 1 before applying the activation function or \"pre-activation\" for short. To calculate this derivative we apply the chain rule of calculus\n",
    "\n",
    "$$\\begin{align}\n",
    "\\delta_2 &= \\frac{\\partial L}{\\partial z_1} \\\\\n",
    "&= \\frac{\\partial L}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1} \\\\\n",
    "&= \\delta_3 \\frac{\\partial z_2}{\\partial a_1}\\frac{\\partial a_1}{\\partial z_1} \\\\\n",
    "&= \\delta_3 W_2 (\\sigma(z_1)(1-\\sigma(z_1)))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With that all of the hard math is done believe it or not. We can easily define all of the gradients we need now\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_3 &= y-\\hat{y} \\\\\n",
    "\\frac{\\partial L}{\\partial W_2} &= \\frac{\\partial L}{\\partial z_2}\\frac{\\partial z_2}{\\partial W2} = a_1^T\\delta_3 \\\\\n",
    "\\frac{\\partial L}{\\partial b_2} &= \\delta_3 \\\\\n",
    "\\delta_2 &= \\delta_3 W_2 (\\sigma(z_1)(1-\\sigma(z_1))) \\\\\n",
    "\\frac{\\partial L}{\\partial W_1} &= \\frac{\\partial L}{\\partial z_1}\\frac{\\partial z_1}{\\partial W_1} = x^T\\delta_2 \\\\\n",
    "\\frac{\\partial L}{\\partial b_1} &= \\delta_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And that's it! With this we can now apply gradient descent to train our network. And not one mention of neurons or any relation to the brain at all :). I believe that it is easier to not think of individual neurons and look at these networks as a series of matrix multiplies and non-linearities. This way it is easier to see the relation to other algorithms such as logistic regression and allows you to look at papers which do delve into math and not be afraid. If you feel this was a bit too rushed and you would like more details than check out some of the suggested material I would reccomend at the end of this tutorial. On to the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self,nb_units,learning_rate,random_seed=1234):\n",
    "        '''\n",
    "        nb_units: The number of units in our hidden layer\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "        random_seed: Seed for our random number generator\n",
    "        \n",
    "        It is always good to seed your random number generator in \n",
    "        your code as neural networks are inherently random and \n",
    "        doing this allows easier reproducibility of your results.\n",
    "        '''\n",
    "        self.W1 = np.random.randn(64, nb_units) / np.sqrt(64)\n",
    "        self.b1 = np.zeros([1,nb_units])\n",
    "        self.W2 = np.random.randn(nb_units, 10) / np.sqrt(nb_units)\n",
    "        self.b2 = np.zeros([1,10])\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def loss(self,X,y):\n",
    "        '''\n",
    "        X: Matrix of data points\n",
    "        y: Matrix of labels in one-hot encoded form\n",
    "        \n",
    "        This function computes our cross-entropy loss. Notice that it\n",
    "        is very expensive to compute this as we have to iterate over\n",
    "        all examples while in backpropogation we only need its derivative\n",
    "        which is much easier to compute\n",
    "        '''\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        probs = softmax(z2)\n",
    "        \n",
    "        log_probs = np.log(probs*y)\n",
    "        log_probs[log_probs == -np.inf] = 0\n",
    "        \n",
    "        loss = -np.sum(log_probs)\n",
    "        return (1./len(X)) * loss\n",
    "    \n",
    "    def accuracy(self,X,y):\n",
    "        '''\n",
    "        X: Matrix of data points\n",
    "        y: Matrix of labels in one-hot encoded form\n",
    "        \n",
    "        A helper function to compute the accuracy of our network\n",
    "        '''\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        for i in xrange(len(y)):\n",
    "            x = X[i,:]\n",
    "            label = np.nonzero(y[i])[0][0]\n",
    "            pred = model.predict(x)\n",
    "            if label == pred:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        return float(correct) / (correct + incorrect)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        '''\n",
    "        x: A vector of pixels for a single example\n",
    "        \n",
    "        Runs our network for a single example to get a prediction\n",
    "        '''\n",
    "        z1 = x.dot(self.W1) + self.b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        probs = softmax(z2)\n",
    "        return np.argmax(probs, axis=1)\n",
    "        \n",
    "    def train(self,X,y,nb_epoch):\n",
    "        '''\n",
    "        X: Matrix of data points\n",
    "        y: Matrix of labels in one-hot encoded form\n",
    "        nb_epoch: The number of epochs to train for\n",
    "        \n",
    "        This function performs the forward propogation and backward \n",
    "        propogation for the given number of epochs\n",
    "        '''\n",
    "        n_training,data_dim = X.shape\n",
    "        for i in xrange(nb_epoch):\n",
    "            for j in xrange(n_training):\n",
    "                x = X[j,:].reshape([1,data_dim])\n",
    "                \n",
    "                #Forward propogation\n",
    "                z1 = x.dot(self.W1) + self.b1\n",
    "                a1 = sigmoid(z1)\n",
    "                z2 = a1.dot(self.W2) + self.b2\n",
    "                probs = softmax(z2)\n",
    "                \n",
    "                #Backpropogation\n",
    "                d3 = -(y[j]-probs)\n",
    "                \n",
    "                dW2 = a1.T.dot(d3)\n",
    "                db2 = np.sum(d3,axis=0,keepdims=True)\n",
    "                \n",
    "                d2 = d3.dot(self.W2.T) * sigmoid(a1,deriv=True)\n",
    "                \n",
    "                dW1 = x.T.dot(d2)\n",
    "                db1 = np.sum(d2,axis=0)\n",
    "                \n",
    "                #Update weights\n",
    "                self.W1 += -lr * dW1\n",
    "                self.b1 += -lr * db1\n",
    "                self.W2 += -lr * dW2\n",
    "                self.b2 += -lr * db2\n",
    "                \n",
    "            #print our loss at the end of each epoch\n",
    "            print \"Loss at epoch {}: {}\".format(i,self.loss(X,y))\n",
    "            \n",
    "        print \"Training complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:35: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.227393185523\n",
      "Loss at epoch 1: 0.124926830203\n",
      "Loss at epoch 2: 0.0849156272638\n",
      "Loss at epoch 3: 0.0621657498182\n",
      "Loss at epoch 4: 0.047015334521\n",
      "Loss at epoch 5: 0.0361425551262\n",
      "Loss at epoch 6: 0.0284811106629\n",
      "Loss at epoch 7: 0.0234222873346\n",
      "Loss at epoch 8: 0.0199735525547\n",
      "Loss at epoch 9: 0.0174566992262\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 10\n",
    "nb_units = 200\n",
    "lr = 0.01\n",
    "\n",
    "#define our model\n",
    "model = NeuralNetwork(nb_units,lr)\n",
    "\n",
    "#train our model\n",
    "model.train(X_train,y_train,nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977777777778\n"
     ]
    }
   ],
   "source": [
    "#test our model\n",
    "print model.accuracy(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Things to Try\n",
    "Below are some exercises you can attempt to confirm you understand the backpropogation algorithm and the different components of a neural network. I also list some \"essential reading\" which goes over some of these methods. I highly reccomend you go over those papers as they will be your bread and butter of sorts.\n",
    "\n",
    "- Play around with the various hyperparameters\n",
    "- Add dropout\n",
    "- Add momentum\n",
    "- Add nesterov momentum\n",
    "- Add l2 regularization\n",
    "- Try better activation functions such as tanh or ReLU\n",
    "- Add support for any number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Reads\n",
    "- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n",
    "- [On the importance of initialization and momentum in deep learning](http://jmlr.org/proceedings/papers/v28/sutskever13.pdf)\n",
    "- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
